{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMSC828P-TopicModelingProject",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "uHIXHTKY-D2l",
        "kTtcroGh-I5U",
        "6WvKLJvx-L4N",
        "667AC4ph-N4L",
        "OO2qrWin-GTN",
        "qH_GRsUOU8Ii",
        "glqjmkZIV4_o",
        "CygD5vHrshp9",
        "1FXstgRyuLgz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/spatkar94/Class-Project/blob/master/CMSC828P_TopicModelingProject.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "5c8_eS279sqc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CMSC 828P (Fall 2018): Topic modeling project"
      ]
    },
    {
      "metadata": {
        "id": "IRlz3g9YDka7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Instructions\n"
      ]
    },
    {
      "metadata": {
        "id": "TXX5wFC20phq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Scope\n",
        "The purpose of this project is to get you familiar with writing code to train topic models and interpret their outputs.\n",
        "\n",
        "This notebook already includes code for downloading and processing a _simulated_ dataset of $N$ \"movie reviews\" over a vocabulary of $L$ words into a matrix $V$ of word counts. Each document also includes an author and a numeric rating from 0-10.\n",
        "\n",
        "You will use open-source Python modules to:\n",
        "\n",
        "1. Estimate the number $K$ of topics (aka the rank) of $V$.\n",
        "2. Train a single topic model for $V$ using the $K$ you chose from (1).\n",
        "3. Evaluate the topics you discover with the \"ground truth\" topics from which the simulated data was generated. The code for evaluating the topics is already provided.\n",
        "4. Identify correlations between the topic mixings/allocations and authorship\n",
        "5. Train supervised learners to predict ratings from documents.\n",
        "\n",
        "Finally, I have also included code for downloading and processing a _real_ movie review dataset at the end of this notebook. The last 10% of your grade will be from an open-ended investigation of this dataset. I look forward to seeing what you can learn!"
      ]
    },
    {
      "metadata": {
        "id": "0uB9IWzR2Sln",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Grading\n",
        "\n",
        "This project will constitute 10% of your final grade. You will be graded on:\n",
        "\n",
        "1. The results you achieve, e.g. can you discover the hidden topics?\n",
        "2. The text you write in between code blocks. This text should explain both what you aim to do AND your interpretation of the results.\n",
        "3. Your code and its readability.\n"
      ]
    },
    {
      "metadata": {
        "id": "IecnYbgyDfs3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Getting started and FAQ\n",
        "\n",
        "Here are two references for using Colaboratory and Jupyter notebooks (from which Colaboratory is based):\n",
        "\n",
        "1. [Using Jupyter notebooks](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/examples_index.html). Examples and explanation of writing/executing code, [Markdown](https://en.wikipedia.org/wiki/Markdown) text (including in \"math mode\"), and other topics.\n",
        "2. [Hello, Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true). This includes an overview of Colaboratory and its key features, as well as helpful code snippets.\n",
        "\n",
        "To get started, execute the existing code in **Setup** and **Download and process simulated movie review dataset** below, and then begin writing your own code in **Topic modeling**."
      ]
    },
    {
      "metadata": {
        "id": "uHIXHTKY-D2l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "id": "kTtcroGh-I5U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install required Python modules"
      ]
    },
    {
      "metadata": {
        "id": "jbwkha_FtyQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1227
        },
        "outputId": "a6a08443-ec8a-4ddf-b67f-8b2b45159cf4"
      },
      "cell_type": "code",
      "source": [
        "!pip install nimfa\n",
        "!pip install gensim\n",
        "!pip install pydrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nimfa\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/f5/830206b70f6f323028b8a164c88530bef06dc9ebbba315aa40c36800a3ed/nimfa-1.3.4.tar.gz (5.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.7MB 847kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from nimfa) (1.14.5)\n",
            "Requirement already satisfied: scipy>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from nimfa) (0.19.1)\n",
            "Building wheels for collected packages: nimfa\n",
            "  Running setup.py bdist_wheel for nimfa ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/63/b7/fd/858ba29683d798fd121b1df0d672943449e64e2ae94fd42af3\n",
            "Successfully built nimfa\n",
            "Installing collected packages: nimfa\n",
            "Successfully installed nimfa-1.3.4\n",
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.5)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 11.2MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/c8/9f4489d72515dfdcd798a71fbebb885820ec60491f92da25045d3b3b5d12/boto3-1.9.7-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 29.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Collecting botocore<1.13.0,>=1.12.7 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/93/25b23c1b853d2a6d5e597eb7ff5b9760f8f7fe98fb4469a6cf1fa9da597d/botocore-1.12.7-py2.py3-none-any.whl (4.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.7MB 5.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 19.7MB/s \n",
            "\u001b[?25hCollecting docutils>=0.10 (from botocore<1.13.0,>=1.12.7->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.7->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Building wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.7 botocore-1.12.7 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n",
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.2)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Running setup.py bdist_wheel for pydrive ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6WvKLJvx-L4N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load dependencies\n",
        "\n",
        "Below I'm loading all the dependencies you will need to execute my code for downloading/processing data, and to help you visualize the data and some results. I'm also loading additional modules that may be of use to you."
      ]
    },
    {
      "metadata": {
        "id": "kB6zg9Xe-Dtm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dependencies for modeling, data download/processing, etc.\n",
        "import os, numpy as np, pandas as pd, urllib.request\n",
        "from collections import defaultdict, Counter\n",
        "from gensim import corpora, models\n",
        "import gensim, nimfa\n",
        "\n",
        "# Load plotting libraries\n",
        "import altair as alt\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from IPython.core.display import display, HTML\n",
        "from scipy.spatial.distance import cdist\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Load tools for evaluating models\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_predict\n",
        "from sklearn.linear_model import ElasticNetCV, LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, explained_variance_score\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Dependencies for downloading data files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "667AC4ph-N4L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set constants and hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "J5rE_F4d-FuS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data files\n",
        "SIM_DATA_FILE = 'sim-movie-review-data-docs.tsv'\n",
        "SIM_TOPICS_FILE = 'sim-movie-review-data-topics.tsv'\n",
        "SIM_DATA_FILE_URL = 'https://drive.google.com/uc?export=download&id=17Ltn4Mzl6PVxBUxpP8oakHWxMq0L3-bb'\n",
        "SIM_TOPICS_FILE_URL = 'https://drive.google.com/uc?export=download&id=1ewxQj2T2g88X7YOahHXi8EP2sJOq2O1y'\n",
        "\n",
        "# Hyperparameters\n",
        "RANDOM_SEED = 57891"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Edgn44F3Pxci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(RANDOM_SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OO2qrWin-GTN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download and process simulated movie review dataset"
      ]
    },
    {
      "metadata": {
        "id": "sbVQ6cSRSg3r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download the data file from the class Google Drive."
      ]
    },
    {
      "metadata": {
        "id": "QO1n9Whbvwmv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the data file\n",
        "_ = urllib.request.urlretrieve(SIM_DATA_FILE_URL, SIM_DATA_FILE)\n",
        "_ = urllib.request.urlretrieve(SIM_TOPICS_FILE_URL, SIM_TOPICS_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wF7llWn1vxAQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we load the topics used to generate the data, which are now in the directory with this notebook. You will use the \"ground truth\" topics to validate what you discover later on. At the same time, we will also load our dataset's vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "j7vKGe8SsYYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3dfc4510-94ff-434c-964e-532d651f1a0b"
      },
      "cell_type": "code",
      "source": [
        "# Load the known topics (for later)\n",
        "with open(SIM_TOPICS_FILE, 'r') as IN:\n",
        "    # Load the file\n",
        "    arrs = [ l.rstrip('\\n').split('\\t') for l in IN ]\n",
        "    header = arrs.pop(0)\n",
        "    \n",
        "    # Parse the vocabulary (first column)\n",
        "    vocabulary = [ arr[0] for arr in arrs ]\n",
        "    N_words = len(vocabulary)\n",
        "    vocab_index = dict(zip(vocabulary, range(N_words)))\n",
        "    \n",
        "    # Parse the true topics\n",
        "    N_true_topics = len(header)-1\n",
        "    true_topics = np.zeros((N_true_topics, len(vocabulary)))\n",
        "    for arr in arrs:\n",
        "        j = vocab_index[arr[0]]\n",
        "        for i in range(N_true_topics):\n",
        "            true_topics[i, j] = float(arr[i+1])\n",
        "            \n",
        "print('- Loaded %s x %s true topic matrix' % true_topics.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Loaded 3 x 195 true topic matrix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g7SkMdVByxjn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, load the given simulated data, which is now in the same directory as this notebook. The input is a tab-separated file with the author name, rating, and list of space-separated (non-stop) words."
      ]
    },
    {
      "metadata": {
        "id": "Sa95xDqOFTYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "37aab56a-f138-4422-8df6-8052455bfb70"
      },
      "cell_type": "code",
      "source": [
        "with open(SIM_DATA_FILE, 'r') as IN:\n",
        "  arrs = [ l.rstrip('\\n').split('\\t') for l in IN if not l.startswith('#')]\n",
        "  \n",
        "  authors = sorted(set( arr[0] for arr in arrs ))\n",
        "  N_authors = len(authors)\n",
        "  author_id = dict(zip(authors, range(N_authors)))\n",
        "  author_ids = [ author_id[arr[0]] for arr in arrs ]\n",
        "  \n",
        "  ratings = [ float(arr[1]) for arr in arrs ]\n",
        "  \n",
        "  texts = [ arr[2].split(' ') for arr in arrs ]\n",
        "  N_docs = len(texts)\n",
        "  \n",
        "print('Loaded %s documents' % N_docs)\n",
        "print('- from a vocabulary of %s words' % N_words)\n",
        "print('- by %s authors' % N_authors)\n",
        "print('- with ratings in [%s-%s]' % (np.min(ratings), np.max(ratings)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 200 documents\n",
            "- from a vocabulary of 195 words\n",
            "- by 3 authors\n",
            "- with ratings in [0.0-10.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p49duWLcS8Bp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we create the document-by-word count matrix using `gensim`."
      ]
    },
    {
      "metadata": {
        "id": "JgGN1RhDVXD_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "373fd792-1cc3-4a94-a897-7731e06bc6e9"
      },
      "cell_type": "code",
      "source": [
        "# Copied from https://www.kaggle.com/canggih/topic-modeling\n",
        "# turn our tokenized documents into a id <-> term dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "print('Vocabulary size: %s' % len(dictionary))\n",
        "\n",
        "# convert tokenized documents into a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "V = gensim.matutils.corpus2dense(corpus, num_terms=len(dictionary))\n",
        "\n",
        "print('Number of words in all documents: %s' % sum(len(d) for d in corpus))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 195\n",
            "Number of words in all documents: 15354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tbeAhlaLNkvr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Plot some simple summaries of our data"
      ]
    },
    {
      "metadata": {
        "id": "lZwGu0d6S7lT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "82fdcea1-b568-4266-fbc4-d4d477895bbc"
      },
      "cell_type": "code",
      "source": [
        "# Plot the thirty most common words\n",
        "word_count = Counter( w for text in texts for w in text )\n",
        "word_count_df = pd.DataFrame([ dict(Word=w, Count=c) for w, c in word_count.most_common(30)])\n",
        "alt.Chart(word_count_df).mark_bar().encode(\n",
        "  x=alt.X('Word', sort=[ w for w, c in word_count.most_common(30) ]),\n",
        "  y='Count'\n",
        ").properties(\n",
        "    title='Thirty most common words'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Chart({\n",
              "  data:     Count           Word\n",
              "  0     980        fantasy\n",
              "  1     899          wacky\n",
              "  2     859         bloody\n",
              "  3     803       dramatic\n",
              "  4     750  disappointing\n",
              "  5     733    second-rate\n",
              "  6     716         brutal\n",
              "  7     711     picaresque\n",
              "  8     692       powerful\n",
              "  9     660    fascinating\n",
              "  10    626        oddball\n",
              "  11    616         tender\n",
              "  12    593         uneven\n",
              "  13    589      senseless\n",
              "  14    584     intriguing\n",
              "  15    573         cliche\n",
              "  16    568    fast-moving\n",
              "  17    524    imaginative\n",
              "  18    521         static\n",
              "  19    516      legendary\n",
              "  20    490         boring\n",
              "  21    485     low-budget\n",
              "  22    479           will\n",
              "  23    479    predictable\n",
              "  24    458          bland\n",
              "  25    457        comical\n",
              "  26    446      enjoyable\n",
              "  27    434           slow\n",
              "  28    428       outdated\n",
              "  29    425        violent,\n",
              "  encoding: EncodingWithFacet({\n",
              "    x: X({\n",
              "      shorthand: 'Word',\n",
              "      sort: ['fantasy', 'wacky', 'bloody', 'dramatic', 'disappointing', 'second-rate', 'brutal', 'picaresque', 'powerful', 'fascinating', 'oddball', 'tender', 'uneven', 'senseless', 'intriguing', 'cliche', 'fast-moving', 'imaginative', 'static', 'legendary', 'boring', 'low-budget', 'will', 'predictable', 'bland', 'comical', 'enjoyable', 'slow', 'outdated', 'violent']\n",
              "    }),\n",
              "    y: Y({\n",
              "      shorthand: 'Count'\n",
              "    })\n",
              "  }),\n",
              "  mark: 'bar',\n",
              "  title: 'Thirty most common words'\n",
              "})"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "  <style>\n",
              "    .vega-actions a {\n",
              "        margin-right: 12px;\n",
              "        color: #757575;\n",
              "        font-weight: normal;\n",
              "        font-size: 13px;\n",
              "    }\n",
              "    .error {\n",
              "        color: red;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "<script src=\"https://cdn.jsdelivr.net/npm//vega@3.3.1\"></script>\n",
              "<script src=\"https://cdn.jsdelivr.net/npm//vega-lite@2.6.0\"></script>\n",
              "<script src=\"https://cdn.jsdelivr.net/npm//vega-embed@3.14\"></script>\n",
              "\n",
              "</head>\n",
              "<body>\n",
              "  <div id=\"vis\"></div>\n",
              "  <script type=\"text/javascript\">\n",
              "    var spec = {\"config\": {\"view\": {\"width\": 400, \"height\": 300}}, \"data\": {\"name\": \"data-adb8cf9a381b1c196073a79e6e67cb4d\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"nominal\", \"field\": \"Word\", \"sort\": [\"fantasy\", \"wacky\", \"bloody\", \"dramatic\", \"disappointing\", \"second-rate\", \"brutal\", \"picaresque\", \"powerful\", \"fascinating\", \"oddball\", \"tender\", \"uneven\", \"senseless\", \"intriguing\", \"cliche\", \"fast-moving\", \"imaginative\", \"static\", \"legendary\", \"boring\", \"low-budget\", \"will\", \"predictable\", \"bland\", \"comical\", \"enjoyable\", \"slow\", \"outdated\", \"violent\"]}, \"y\": {\"type\": \"quantitative\", \"field\": \"Count\"}}, \"title\": \"Thirty most common words\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"datasets\": {\"data-adb8cf9a381b1c196073a79e6e67cb4d\": [{\"Count\": 980, \"Word\": \"fantasy\"}, {\"Count\": 899, \"Word\": \"wacky\"}, {\"Count\": 859, \"Word\": \"bloody\"}, {\"Count\": 803, \"Word\": \"dramatic\"}, {\"Count\": 750, \"Word\": \"disappointing\"}, {\"Count\": 733, \"Word\": \"second-rate\"}, {\"Count\": 716, \"Word\": \"brutal\"}, {\"Count\": 711, \"Word\": \"picaresque\"}, {\"Count\": 692, \"Word\": \"powerful\"}, {\"Count\": 660, \"Word\": \"fascinating\"}, {\"Count\": 626, \"Word\": \"oddball\"}, {\"Count\": 616, \"Word\": \"tender\"}, {\"Count\": 593, \"Word\": \"uneven\"}, {\"Count\": 589, \"Word\": \"senseless\"}, {\"Count\": 584, \"Word\": \"intriguing\"}, {\"Count\": 573, \"Word\": \"cliche\"}, {\"Count\": 568, \"Word\": \"fast-moving\"}, {\"Count\": 524, \"Word\": \"imaginative\"}, {\"Count\": 521, \"Word\": \"static\"}, {\"Count\": 516, \"Word\": \"legendary\"}, {\"Count\": 490, \"Word\": \"boring\"}, {\"Count\": 485, \"Word\": \"low-budget\"}, {\"Count\": 479, \"Word\": \"will\"}, {\"Count\": 479, \"Word\": \"predictable\"}, {\"Count\": 458, \"Word\": \"bland\"}, {\"Count\": 457, \"Word\": \"comical\"}, {\"Count\": 446, \"Word\": \"enjoyable\"}, {\"Count\": 434, \"Word\": \"slow\"}, {\"Count\": 428, \"Word\": \"outdated\"}, {\"Count\": 425, \"Word\": \"violent\"}]}};\n",
              "    var embed_opt = {\"mode\": \"vega-lite\"};\n",
              "\n",
              "    function showError(el, error){\n",
              "        el.innerHTML = ('<div class=\"error\">'\n",
              "                        + '<p>JavaScript Error: ' + error.message + '</p>'\n",
              "                        + \"<p>This usually means there's a typo in your chart specification. \"\n",
              "                        + \"See the javascript console for the full traceback.</p>\"\n",
              "                        + '</div>');\n",
              "        throw error;\n",
              "    }\n",
              "    const el = document.getElementById('vis');\n",
              "    vegaEmbed(\"#vis\", spec, embed_opt)\n",
              "      .catch(error => showError(el, error));\n",
              "  </script>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "MZVDQX11NrCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "3c68f851-734b-492f-ec98-3a06de5dc88d"
      },
      "cell_type": "code",
      "source": [
        "ratings_df = pd.DataFrame([ dict(Rating=r, Count=c) for r, c in Counter(ratings).items() ])\n",
        "alt.Chart(ratings_df).mark_bar().encode(\n",
        "  x='Rating:N',\n",
        "  y='Count',\n",
        ").properties(\n",
        "    title='Distribution of ratings'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Chart({\n",
              "  data:    Count  Rating\n",
              "  0     15    10.0\n",
              "  1     37     3.0\n",
              "  2     31     2.0\n",
              "  3     31     8.0\n",
              "  4     36     7.0\n",
              "  5     18     9.0\n",
              "  6      9     0.0\n",
              "  7     23     1.0,\n",
              "  encoding: EncodingWithFacet({\n",
              "    x: X({\n",
              "      shorthand: 'Rating:N'\n",
              "    }),\n",
              "    y: Y({\n",
              "      shorthand: 'Count'\n",
              "    })\n",
              "  }),\n",
              "  mark: 'bar',\n",
              "  title: 'Distribution of ratings'\n",
              "})"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "  <style>\n",
              "    .vega-actions a {\n",
              "        margin-right: 12px;\n",
              "        color: #757575;\n",
              "        font-weight: normal;\n",
              "        font-size: 13px;\n",
              "    }\n",
              "    .error {\n",
              "        color: red;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "<script src=\"https://cdn.jsdelivr.net/npm//vega@3.3.1\"></script>\n",
              "<script src=\"https://cdn.jsdelivr.net/npm//vega-lite@2.6.0\"></script>\n",
              "<script src=\"https://cdn.jsdelivr.net/npm//vega-embed@3.14\"></script>\n",
              "\n",
              "</head>\n",
              "<body>\n",
              "  <div id=\"vis\"></div>\n",
              "  <script type=\"text/javascript\">\n",
              "    var spec = {\"config\": {\"view\": {\"width\": 400, \"height\": 300}}, \"data\": {\"name\": \"data-f1c86cbbcb3e27df6e8d4b49912d6f11\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"nominal\", \"field\": \"Rating\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Count\"}}, \"title\": \"Distribution of ratings\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"datasets\": {\"data-f1c86cbbcb3e27df6e8d4b49912d6f11\": [{\"Count\": 15, \"Rating\": 10.0}, {\"Count\": 37, \"Rating\": 3.0}, {\"Count\": 31, \"Rating\": 2.0}, {\"Count\": 31, \"Rating\": 8.0}, {\"Count\": 36, \"Rating\": 7.0}, {\"Count\": 18, \"Rating\": 9.0}, {\"Count\": 9, \"Rating\": 0.0}, {\"Count\": 23, \"Rating\": 1.0}]}};\n",
              "    var embed_opt = {\"mode\": \"vega-lite\"};\n",
              "\n",
              "    function showError(el, error){\n",
              "        el.innerHTML = ('<div class=\"error\">'\n",
              "                        + '<p>JavaScript Error: ' + error.message + '</p>'\n",
              "                        + \"<p>This usually means there's a typo in your chart specification. \"\n",
              "                        + \"See the javascript console for the full traceback.</p>\"\n",
              "                        + '</div>');\n",
              "        throw error;\n",
              "    }\n",
              "    const el = document.getElementById('vis');\n",
              "    vegaEmbed(\"#vis\", spec, embed_opt)\n",
              "      .catch(error => showError(el, error));\n",
              "  </script>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "qH_GRsUOU8Ii",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Topic modeling\n",
        "\n",
        "In this section, you will pick your favorite topic modeling algorithm and use it to discover topics on the movie review dataset. No matter which approach you use, you will need to come up with a procedure for selecting the number $K$ of topics. I suggest you limit your search space for $K$ to 2-10 topics.\n",
        "\n",
        "**Implementations**. If you take a probabilistic topic modeling approach, I suggest you use [`gensim`](https://radimrehurek.com/gensim/). If you take a non-negative matrix factorization (NMF) approach, I suggest you use [`nimfa`](http://nimfa.biolab.si/index.html)."
      ]
    },
    {
      "metadata": {
        "id": "Klrm_abIVnxJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Select $K$ and learn topics\n",
        "\n",
        "This part has two steps.\n",
        "\n",
        "1. Use a method discussed in class to estimate the number $K$ of topics (aka the estimated rank) in this dataset.\n",
        "2.  Train one model on all the data using the $K$ you found above.\n",
        "\n",
        "Once you've trained the model, create a $K\\times L$ \"topic\" matrix $P$, and a $N \\times K$ \"mixings\" matrix $E$. Don't forget to normalize $P$ so the rows are probability distributions (and renormalize $E$ accordingly)"
      ]
    },
    {
      "metadata": {
        "id": "9kmwYDyh31YO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "migEuS9qVqOJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Evaluate your topics\n",
        "\n",
        "Now that you've chosen $K$ (the number of topics or rank) and trained a single model, we are going to evaluate the topics you've found in two ways.\n",
        "\n",
        "1. Create a table with the top ten words per topic.\n",
        "2. Compare the cosine similarity of the discovered topics with the true, hidden topics.\n",
        "\n",
        "Below, I've provided the code for both these evaluations, and assume that you've defined an $K\\times L$ topic matrix $P$ above."
      ]
    },
    {
      "metadata": {
        "id": "3sbNCGV2Vqeh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Take top N words per \"topic\"\n",
        "top_n_words = 10\n",
        "tbl = '<table><tr><th>Topic</th>%s</tr>' % ''.join('<th>Word %s</th>' % (i+1) for i in range(top_n_words))\n",
        "for i in range(estimated_rank):\n",
        "    indices = sorted(dictionary.keys(), key=lambda j: P[i, j], reverse=True)[:top_n_words]\n",
        "    tbl += '<tr><td>Topic %s</td>' % (i+1)\n",
        "    for idx in indices:\n",
        "        tbl += '<td>%s</td>' % dictionary[idx]\n",
        "    tbl += '</tr>'\n",
        "tbl += '</table>'\n",
        "\n",
        "display(HTML(tbl))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4h2P_6afsYY9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity between true topics and discovered topics\n",
        "reordered_vocab = [ vocab_index[w] for w in dictionary.values() ]\n",
        "_ = sns.heatmap(1.-cdist(P, true_topics[:, reordered_vocab], metric='cosine'), annot=True)\n",
        "_ = plt.xlabel('Inferred topic')\n",
        "_ = plt.ylabel('True topic')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "glqjmkZIV4_o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation with additional data\n",
        "\n",
        "In this section, you will analyze your topics using additional data.\n"
      ]
    },
    {
      "metadata": {
        "id": "ags2z75BXTPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Topics by author\n",
        "\n",
        "First, are there differences in topics depending on the author of the review? One way to find out is to cluster documents by topic and see if there are associations with authors. You can do this using a visualization, statistical test, or both."
      ]
    },
    {
      "metadata": {
        "id": "KrmnRBRc4D71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vKk8oy9aXn_V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Document ratings\n",
        "\n",
        "Second, train a supervised learner to predict document ratings from the learned topic mixings, and evaluate your regressor on held-out data. You can use any supervised learning algorithm you like (e.g. simple linear regression, support vector machine, random forest, etc). How do the results compare to when you train your supervised learner on word counts from the document-by-word matrix (i.e. without using topics)?"
      ]
    },
    {
      "metadata": {
        "id": "AdegZp1Y3-4B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CygD5vHrshp9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Application to real data\n",
        "\n",
        "For the last 10% of your grade on this assignment, I am asking you to develop topic models for a real life movie review dataset.\n",
        "\n",
        "Below, I include the code for downloading and processing the dataset. Your task is to then do an open-ended investigation of this dataset and report back. Some questions you can try and answer:\n",
        "\n",
        "*   How many topics are there in this dataset?\n",
        "*   Can you predict movie ratings using bag-of-words featurization? How about topic mixings?\n",
        "*   Are some reviewers more/less likely to write about certain topics?\n",
        "\n",
        "Or any other questions you find interesting! "
      ]
    },
    {
      "metadata": {
        "id": "sFVzn5yUtWnt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup additional resources"
      ]
    },
    {
      "metadata": {
        "id": "-_GVaIOJtdJd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install stop_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TlghEBSKtO1d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from stop_words import get_stop_words\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XYeruqqTtWCk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "_ = nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1FXstgRyuLgz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Constants/hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "qW6UEekzuAwu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# File/URL constants\n",
        "REAL_DATA_TAR_URL = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz'\n",
        "REAL_DATA_TAR = \"movie-review-data.tar.gz\"\n",
        "\n",
        "# Dataset constants\n",
        "RATING_SCALE = 4\n",
        "VOCAB_SIZE = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHbQhIPHuAnT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download and process movie review dataset\n",
        "\n",
        "We download the movie review data analyzed in the Supervised LDA paper (Blei & McAuliffe. _NIPS_, 2008) from http://www.cs.cornell.edu/people/pabo/movie-review-data/."
      ]
    },
    {
      "metadata": {
        "id": "x1g-5ekKuE90",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the data file\n",
        "urllib.request.urlretrieve(REAL_DATA_TAR_URL, REAL_DATA_TAR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SuUfmjpxuQ66",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The main processing we do is removing stop words and infrequent words. We also stem the words. This is largely based on the processing from https://www.kaggle.com/canggih/topic-modeling, and in some cases the code is copied directly."
      ]
    },
    {
      "metadata": {
        "id": "6j06kO6MuRNY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setup tokenizer\n",
        "# copied from https://www.kaggle.com/canggih/topic-modeling\n",
        "pattern = r'\\b[^\\d\\W]+\\b'\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "en_stop = get_stop_words('en')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "welNjNdSuSrn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a map of authors to ordered lists of documents and ratings\n",
        "docs = defaultdict(list)\n",
        "ratings_map = defaultdict(list)\n",
        "with tarfile.open(REAL_DATA_TAR) as TAR:\n",
        "  for member in TAR:\n",
        "    basename = os.path.basename(member.name)\n",
        "    if basename.startswith('subj') or basename.startswith('label.%sclass' % RATING_SCALE):\n",
        "      author = basename.split('.')[-1]\n",
        "      if basename.startswith('subj'):              \n",
        "        # Extract each review as a separate document\n",
        "        for line in TAR.extractfile(member):\n",
        "          # clean and tokenize document string\n",
        "          # code below is copied from https://www.kaggle.com/canggih/topic-modeling\n",
        "          raw = line.decode(\"utf-8\", \"replace\").rstrip('\\n').lower()\n",
        "          tokens = tokenizer.tokenize(raw)\n",
        "\n",
        "          # remove stop words from tokens\n",
        "          stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n",
        "\n",
        "          # lemmatize tokens\n",
        "          lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens]\n",
        "\n",
        "          # remove word containing only single char\n",
        "          new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n",
        "\n",
        "          # add tokens to list for author\n",
        "          docs[author].append( new_lemma_tokens )\n",
        "\n",
        "      # Process ratings\n",
        "      else:\n",
        "        for line in TAR.extractfile(member):\n",
        "          ratings_map[author].append( int(line.decode(\"utf-8\", \"replace\").rstrip('\\n')) )\n",
        "          \n",
        "    # Otherwise skip\n",
        "    else:\n",
        "      continue\n",
        "      \n",
        "# Summarize the data\n",
        "N_docs = sum( len(ds) for ds in docs.values() )\n",
        "N_authors = len(docs)\n",
        "print('Loaded %s documents from %s authors' % (N_docs, N_authors))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a1I5oBizueuy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we create the document-by-word count matrix using `gensim`."
      ]
    },
    {
      "metadata": {
        "id": "KJhhA4MaufBZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Flatten out all our data for easy use later on\n",
        "authors = sorted(docs.keys())\n",
        "author_id = dict(zip(authors, range(N_authors)))\n",
        "raw_texts = [ d for a in authors for d in docs[a] ]\n",
        "ratings = [ r for a in authors for r in ratings_map[a] ]\n",
        "author_ids = [ [author_id[a]] * len(docs[a]) for a in authors ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pSpYUlSPuuhv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Restrict to 500 words according to TF-IDF:"
      ]
    },
    {
      "metadata": {
        "id": "gqZ4r_uTui-U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vect = TfidfVectorizer(min_df=3, max_df=0.5, max_features=VOCAB_SIZE).fit([ ' '.join(text) for text in raw_texts ])\n",
        "vocab_set = set(vect.vocabulary_)\n",
        "texts = [ [ w for w in text if w in vocab_set] for text in raw_texts ]\n",
        "print('New vocabulary length: %s' % len(vocab_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RRWwQ-HSuzFk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copied from https://www.kaggle.com/canggih/topic-modeling\n",
        "# turn our tokenized documents into a id <-> term dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "print('Vocabulary size: %s' % len(dictionary))\n",
        "\n",
        "# convert tokenized documents into a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "V = gensim.matutils.corpus2dense(corpus, num_terms=len(dictionary))\n",
        "\n",
        "print('Number of words in all documents: %s' % sum(len(d) for d in corpus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Tfe9FeWvAtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Your analysis of real data\n",
        "\n",
        "Fill in below."
      ]
    },
    {
      "metadata": {
        "id": "QywfPhRau6xk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}